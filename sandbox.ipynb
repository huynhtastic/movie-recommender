{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7d6659f0c9b0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.setMaster('local[*]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7d6659f0c9b0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.setAppName('sandbox')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=sandbox>\n"
     ]
    }
   ],
   "source": [
    "print(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PACKAGE_EXTENSIONS',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__enter__',\n",
       " '__eq__',\n",
       " '__exit__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_accumulatorServer',\n",
       " '_active_spark_context',\n",
       " '_batchSize',\n",
       " '_callsite',\n",
       " '_checkpointFile',\n",
       " '_conf',\n",
       " '_dictToJavaMap',\n",
       " '_do_init',\n",
       " '_ensure_initialized',\n",
       " '_gateway',\n",
       " '_getJavaStorageLevel',\n",
       " '_initialize_context',\n",
       " '_javaAccumulator',\n",
       " '_jsc',\n",
       " '_jvm',\n",
       " '_lock',\n",
       " '_next_accum_id',\n",
       " '_pickled_broadcast_vars',\n",
       " '_python_includes',\n",
       " '_repr_html_',\n",
       " '_serialize_to_jvm',\n",
       " '_temp_dir',\n",
       " '_unbatched_serializer',\n",
       " 'accumulator',\n",
       " 'addFile',\n",
       " 'addPyFile',\n",
       " 'appName',\n",
       " 'applicationId',\n",
       " 'binaryFiles',\n",
       " 'binaryRecords',\n",
       " 'broadcast',\n",
       " 'cancelAllJobs',\n",
       " 'cancelJobGroup',\n",
       " 'defaultMinPartitions',\n",
       " 'defaultParallelism',\n",
       " 'dump_profiles',\n",
       " 'emptyRDD',\n",
       " 'environment',\n",
       " 'getConf',\n",
       " 'getLocalProperty',\n",
       " 'getOrCreate',\n",
       " 'hadoopFile',\n",
       " 'hadoopRDD',\n",
       " 'master',\n",
       " 'newAPIHadoopFile',\n",
       " 'newAPIHadoopRDD',\n",
       " 'parallelize',\n",
       " 'pickleFile',\n",
       " 'profiler_collector',\n",
       " 'pythonExec',\n",
       " 'pythonVer',\n",
       " 'range',\n",
       " 'runJob',\n",
       " 'sequenceFile',\n",
       " 'serializer',\n",
       " 'setCheckpointDir',\n",
       " 'setJobDescription',\n",
       " 'setJobGroup',\n",
       " 'setLocalProperty',\n",
       " 'setLogLevel',\n",
       " 'setSystemProperty',\n",
       " 'show_profiles',\n",
       " 'sparkHome',\n",
       " 'sparkUser',\n",
       " 'startTime',\n",
       " 'statusTracker',\n",
       " 'stop',\n",
       " 'textFile',\n",
       " 'uiWebUrl',\n",
       " 'union',\n",
       " 'version',\n",
       " 'wholeTextFiles']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "textFile() missing 1 required positional argument: 'name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-43faa2a4ed0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtextFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: textFile() missing 1 required positional argument: 'name'"
     ]
    }
   ],
   "source": [
    "sc.textFile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_df = sc.textFile('tags.csv')\n",
    "ratings_df = sc.textFile('./ratings.csv')\n",
    "movies_df = sc.textFile('./movies.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tags.csv MapPartitionsRDD[5] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__add__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getnewargs__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_computeFractionForSampleSize',\n",
       " '_defaultReducePartitions',\n",
       " '_id',\n",
       " '_jrdd',\n",
       " '_jrdd_deserializer',\n",
       " '_memory_limit',\n",
       " '_pickled',\n",
       " '_reserialize',\n",
       " '_to_java_object_rdd',\n",
       " 'aggregate',\n",
       " 'aggregateByKey',\n",
       " 'cache',\n",
       " 'cartesian',\n",
       " 'checkpoint',\n",
       " 'coalesce',\n",
       " 'cogroup',\n",
       " 'collect',\n",
       " 'collectAsMap',\n",
       " 'combineByKey',\n",
       " 'context',\n",
       " 'count',\n",
       " 'countApprox',\n",
       " 'countApproxDistinct',\n",
       " 'countByKey',\n",
       " 'countByValue',\n",
       " 'ctx',\n",
       " 'distinct',\n",
       " 'filter',\n",
       " 'first',\n",
       " 'flatMap',\n",
       " 'flatMapValues',\n",
       " 'fold',\n",
       " 'foldByKey',\n",
       " 'foreach',\n",
       " 'foreachPartition',\n",
       " 'fullOuterJoin',\n",
       " 'getCheckpointFile',\n",
       " 'getNumPartitions',\n",
       " 'getStorageLevel',\n",
       " 'glom',\n",
       " 'groupBy',\n",
       " 'groupByKey',\n",
       " 'groupWith',\n",
       " 'histogram',\n",
       " 'id',\n",
       " 'intersection',\n",
       " 'isCheckpointed',\n",
       " 'isEmpty',\n",
       " 'isLocallyCheckpointed',\n",
       " 'is_cached',\n",
       " 'is_checkpointed',\n",
       " 'join',\n",
       " 'keyBy',\n",
       " 'keys',\n",
       " 'leftOuterJoin',\n",
       " 'localCheckpoint',\n",
       " 'lookup',\n",
       " 'map',\n",
       " 'mapPartitions',\n",
       " 'mapPartitionsWithIndex',\n",
       " 'mapPartitionsWithSplit',\n",
       " 'mapValues',\n",
       " 'max',\n",
       " 'mean',\n",
       " 'meanApprox',\n",
       " 'min',\n",
       " 'name',\n",
       " 'partitionBy',\n",
       " 'partitioner',\n",
       " 'persist',\n",
       " 'pipe',\n",
       " 'randomSplit',\n",
       " 'reduce',\n",
       " 'reduceByKey',\n",
       " 'reduceByKeyLocally',\n",
       " 'repartition',\n",
       " 'repartitionAndSortWithinPartitions',\n",
       " 'rightOuterJoin',\n",
       " 'sample',\n",
       " 'sampleByKey',\n",
       " 'sampleStdev',\n",
       " 'sampleVariance',\n",
       " 'saveAsHadoopDataset',\n",
       " 'saveAsHadoopFile',\n",
       " 'saveAsNewAPIHadoopDataset',\n",
       " 'saveAsNewAPIHadoopFile',\n",
       " 'saveAsPickleFile',\n",
       " 'saveAsSequenceFile',\n",
       " 'saveAsTextFile',\n",
       " 'setName',\n",
       " 'sortBy',\n",
       " 'sortByKey',\n",
       " 'stats',\n",
       " 'stdev',\n",
       " 'subtract',\n",
       " 'subtractByKey',\n",
       " 'sum',\n",
       " 'sumApprox',\n",
       " 'take',\n",
       " 'takeOrdered',\n",
       " 'takeSample',\n",
       " 'toDebugString',\n",
       " 'toLocalIterator',\n",
       " 'top',\n",
       " 'treeAggregate',\n",
       " 'treeReduce',\n",
       " 'union',\n",
       " 'unpersist',\n",
       " 'values',\n",
       " 'variance',\n",
       " 'zip',\n",
       " 'zipWithIndex',\n",
       " 'zipWithUniqueId']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tags_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['userId,movieId,tag,timestamp',\n",
       " \"15,339,sandra 'boring' bullock,1138537770\",\n",
       " '15,1955,dentist,1193435061',\n",
       " '15,7478,Cambodia,1170560997',\n",
       " '15,32892,Russian,1170626366']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import starmap\n",
    "\n",
    "tags_dep = tags_df.map(lambda line: line.split(','))\n",
    "ratings_dep = ratings_df.map(lambda line: line.split(','))\n",
    "movies_dep = movies_df.map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['userId', 'movieId', 'tag', 'timestamp'],\n",
       " ['15', '339', \"sandra 'boring' bullock\", '1138537770'],\n",
       " ['15', '1955', 'dentist', '1193435061'],\n",
       " ['15', '7478', 'Cambodia', '1170560997'],\n",
       " ['15', '32892', 'Russian', '1170626366'],\n",
       " ['15', '34162', 'forgettable', '1141391765'],\n",
       " ['15', '35957', 'short', '1141391873'],\n",
       " ['15', '37729', 'dull story', '1141391806'],\n",
       " ['15', '45950', 'powerpoint', '1169616291'],\n",
       " ['15', '100365', 'activist', '1425876220']]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_dep.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['userId', 'movieId', 'rating', 'timestamp'],\n",
       " ['1', '31', '2.5', '1260759144'],\n",
       " ['1', '1029', '3.0', '1260759179'],\n",
       " ['1', '1061', '3.0', '1260759182'],\n",
       " ['1', '1129', '2.0', '1260759185'],\n",
       " ['1', '1172', '4.0', '1260759205'],\n",
       " ['1', '1263', '2.0', '1260759151'],\n",
       " ['1', '1287', '2.0', '1260759187'],\n",
       " ['1', '1293', '2.0', '1260759148'],\n",
       " ['1', '1339', '3.5', '1260759125']]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratings_dep.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['movieId,title,genres',\n",
       " '1,Toy Story (1995),Adventure|Animation|Children|Comedy|Fantasy',\n",
       " '2,Jumanji (1995),Adventure|Children|Fantasy',\n",
       " '3,Grumpier Old Men (1995),Comedy|Romance',\n",
       " '4,Waiting to Exhale (1995),Comedy|Drama|Romance',\n",
       " '5,Father of the Bride Part II (1995),Comedy',\n",
       " '6,Heat (1995),Action|Crime|Thriller',\n",
       " '7,Sabrina (1995),Comedy|Romance',\n",
       " '8,Tom and Huck (1995),Adventure|Children',\n",
       " '9,Sudden Death (1995),Action']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df.take(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_header = movies_df.first()\n",
    "movies_rdd = movies_df.filter(lambda line: line != movies_header)\n",
    "# movies_rdd = movies_df.map(lambda x: int(x[0]))\n",
    "movies_rdd = movies_rdd.map(lambda line: line.split(',')).map(lambda x: (int(x[0]), (x[1], set(x[2].split('|')))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(4, ('Waiting to Exhale (1995)', {'Comedy', 'Drama', 'Romance'})),\n",
       " (14, ('Nixon (1995)', {'Drama'})),\n",
       " (16, ('Casino (1995)', {'Crime', 'Drama'})),\n",
       " (17, ('Sense and Sensibility (1995)', {'Drama', 'Romance'})),\n",
       " (20,\n",
       "  ('Money Train (1995)', {'Action', 'Comedy', 'Crime', 'Drama', 'Thriller'})),\n",
       " (22, ('Copycat (1995)', {'Crime', 'Drama', 'Horror', 'Mystery', 'Thriller'})),\n",
       " (24, ('Powder (1995)', {'Drama', 'Sci-Fi'})),\n",
       " (25, ('Leaving Las Vegas (1995)', {'Drama', 'Romance'})),\n",
       " (26, ('Othello (1995)', {'Drama'})),\n",
       " (27, ('Now and Then (1995)', {'Children', 'Drama'}))]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_movie = (31, ('Dangerous Minds (1995)', {'Drama'}))\n",
    "#movies_rdd.filter(lambda x: x[0] == a).first()\n",
    "movies_rdd.filter(lambda movie: movie[1][1].intersection(src_movie[1][1])).take(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
